library(ISLR)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary)) # Finds count of missing values
# na.omit() function removes missing values
Hitters=na.omit(Hitters)
dim(Hitters)
head(Hitters)
row.names(Hitters)
sum(is.na(Hitters))

## Best Subset Selection
# regsubsets() in the leaps library works like lm()
# but finds the best subset for K predictors
# install.packages("leaps")
library(leaps)
regfit.full=regsubsets(Salary~.,Hitters) # By default stops at 8 predictors
summary(regfit.full)
regfit.full=regsubsets(Salary~.,data=Hitters,nvmax=19) # sets 19 predictors as the maximum
reg.summary=summary(regfit.full)
names(reg.summary)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",
     type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
# points() puts point on an existing plot
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11],col="red",cex=2,pch=20)
which.min(reg.summary$bic)
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type="l")
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)
# This library has a built in plot function that is nice
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
?plot.regsubsets
# Use coef() to get the estimates for a given fit
coef(regfit.full,6)

## Forward and Backward Selection
# We can also use regsubsets() to use forward or backwards stepwise selection
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=10,method="forward")
summary(regfit.fwd)
regfit.bwd=regsubsets(Salary~.,data=Hitters,nvmax=10,method="backward")
summary(regfit.bwd)
# For 7 predictors, the best subsets differs from the stepwise predictors
coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)

## Choosing among models - Validation Set
# Validation Set
set.seed(1)
train=sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)
test=(!train)
regfit.best=regsubsets(Salary~.,data=Hitters[train,],nvmax=19)
test.mat=model.matrix(Salary~.,data=Hitters[test,]) # builds an X matrix from data holding coefficients
# from models of difference sizes.  We extract these.
val.errors=rep(NA,19)
for(i in 1:19){
  coefi=coef(regfit.best,id=i)
  pred=test.mat[,names(coefi)]%*%coefi # Multiplies the coefficients and adds them (Matrix Multiplication)
  val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
which.min(val.errors) # 10 predictors is the best model found
coef(regfit.best,10)
# This was a bit tedious because predict() isn't available, we can create a function for future use.
# Creating our own predict function:
predict.regsubsets=function(object,newdata,id){
  form=as.formula(object$call[[2]]) # Extracting the formula
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
# Once you choose the best model (10 predictors in this case), always fit for coefficients on the full dataset
regfit.best=regsubsets(Salary~.,data=Hitters,nvmax=19)
coef(regfit.best,10)

## Choosing among models - Cross Validation
# This approach is somewhat involved; we do best subset selection within each k folds
# First allocate observations to the 10 folds
k=10
set.seed(1)
folds=sample(1:k,nrow(Hitters),replace=TRUE)
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))

for(j in 1:k){
  best.fit=regsubsets(Salary~.,data=Hitters[folds!=j,],nvmax=19)
  for(i in 1:19){
    pred=predict.regsubsets(best.fit,Hitters[folds==j,],id=i)
    cv.errors[j,i]=mean( (Hitters$Salary[folds==j]-pred)^2)
  }
}
# 10 x 19 matrix, where you have 19 flexibilities and 10 folds.  
cv.errors 
# We want 1 x 19 so we use apply()
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
which.min(mean.cv.errors)
points(11,mean.cv.errors[11],col="red",cex=2,pch=20)
# We now want to now get coefficients for the 11 predictor model on all the data
reg.best=regsubsets(Salary~.,data=Hitters, nvmax=19)
coef(reg.best,11)

## Ridge Regression and the Lasso
# glmnet package can perform ridge regression and lasso.  glmnet() is the main function in the package
# glmnet is also good in that it converts qualitative variables into dummy variables (also input must be numeric or these dummy qualitative values)
# install.packages("glmnet")
library(glmnet)
dim(Hitters)
Hitters=na.omit(Hitters)
dim(Hitters)
# model.matrix is important
x=model.matrix(Salary~.,Hitters)[,-1] # Removes intercept
y=Hitters$Salary
# alpha=0 is ridge regression model fit; alpha=1 is lasso model fit
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
# grid is generated by default, but we use it in the lab
# by default standardize=TRUE for glmnet()
dim(coef(ridge.mod)) # 20 x 100.  p=20, lambda values vector is length 100 (one value per lambda)
# We expect low coefficients when the lambda (part of the penalty term) is large
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
# lambda at index 60 is smaller, so coefficients are larger
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))

