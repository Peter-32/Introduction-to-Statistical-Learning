library(ISLR)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters$Salary)) # Finds count of missing values
# na.omit() function removes missing values
Hitters=na.omit(Hitters)
dim(Hitters)
head(Hitters)
row.names(Hitters)
sum(is.na(Hitters))

## Best Subset Selection
# regsubsets() in the leaps library works like lm()
# but finds the best subset for K predictors
# install.packages("leaps")
library(leaps)
regfit.full=regsubsets(Salary~.,Hitters) # By default stops at 8 predictors
summary(regfit.full)
regfit.full=regsubsets(Salary~.,data=Hitters,nvmax=19) # sets 19 predictors as the maximum
reg.summary=summary(regfit.full)
names(reg.summary)
reg.summary$rsq
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",
     type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
# points() puts point on an existing plot
which.max(reg.summary$adjr2)
points(11,reg.summary$adjr2[11],col="red",cex=2,pch=20)
which.min(reg.summary$bic)
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type="l")
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)
# This library has a built in plot function that is nice
plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")
?plot.regsubsets
# Use coef() to get the estimates for a given fit
coef(regfit.full,6)

## Forward and Backward Selection
# We can also use regsubsets() to use forward or backwards stepwise selection
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=10,method="forward")
summary(regfit.fwd)
regfit.bwd=regsubsets(Salary~.,data=Hitters,nvmax=10,method="backward")
summary(regfit.bwd)
# For 7 predictors, the best subsets differs from the stepwise predictors
coef(regfit.full,7)
coef(regfit.fwd,7)
coef(regfit.bwd,7)

## Choosing among models - Validation Set
# Validation Set
set.seed(1)
train=sample(c(TRUE,FALSE), nrow(Hitters),rep=TRUE)
test=(!train)
regfit.best=regsubsets(Salary~.,data=Hitters[train,],nvmax=19)
test.mat=model.matrix(Salary~.,data=Hitters[test,]) # builds an X matrix from data holding coefficients
# from models of difference sizes.  We extract these.
val.errors=rep(NA,19)
for(i in 1:19){
  coefi=coef(regfit.best,id=i)
  pred=test.mat[,names(coefi)]%*%coefi # Multiplies the coefficients and adds them (Matrix Multiplication)
  val.errors[i]=mean((Hitters$Salary[test]-pred)^2)
}
which.min(val.errors) # 10 predictors is the best model found
coef(regfit.best,10)
# This was a bit tedious because predict() isn't available, we can create a function for future use.
# Creating our own predict function:
predict.regsubsets=function(object,newdata,id){
  form=as.formula(object$call[[2]]) # Extracting the formula
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}
# Once you choose the best model (10 predictors in this case), always fit for coefficients on the full dataset
regfit.best=regsubsets(Salary~.,data=Hitters,nvmax=19)
coef(regfit.best,10)

## Choosing among models - Cross Validation
# This approach is somewhat involved; we do best subset selection within each k folds
# First allocate observations to the 10 folds
k=10
set.seed(1)
folds=sample(1:k,nrow(Hitters),replace=TRUE)
cv.errors=matrix(NA,k,19, dimnames=list(NULL, paste(1:19)))

for(j in 1:k){
  best.fit=regsubsets(Salary~.,data=Hitters[folds!=j,],nvmax=19)
  for(i in 1:19){
    pred=predict.regsubsets(best.fit,Hitters[folds==j,],id=i)
    cv.errors[j,i]=mean( (Hitters$Salary[folds==j]-pred)^2)
  }
}
# 10 x 19 matrix, where you have 19 flexibilities and 10 folds.  
cv.errors 
# We want 1 x 19 so we use apply()
mean.cv.errors=apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type='b')
which.min(mean.cv.errors)
points(11,mean.cv.errors[11],col="red",cex=2,pch=20)
# We now want to now get coefficients for the 11 predictor model on all the data
reg.best=regsubsets(Salary~.,data=Hitters, nvmax=19)
coef(reg.best,11)

## Ridge Regression and the Lasso
# glmnet package can perform ridge regression and lasso.  glmnet() is the main function in the package
# glmnet is also good in that it converts qualitative variables into dummy variables (also input must be numeric or these dummy qualitative values)
# install.packages("glmnet")
# MSE with one set
library(glmnet)
dim(Hitters)
Hitters=na.omit(Hitters)
dim(Hitters)
# model.matrix is important
x=model.matrix(Salary~.,Hitters)[,-1] # Removes intercept
y=Hitters$Salary
# alpha=0 is ridge regression model fit; alpha=1 is lasso model fit
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
# grid is generated by default, but we use it in the lab
# by default standardize=TRUE for glmnet()
dim(coef(ridge.mod)) # 20 x 100.  p=20, lambda values vector is length 100 (one value per lambda)
# We expect low coefficients when the lambda (part of the penalty term) is large
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
# lambda at index 60 is smaller, so coefficients are larger
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
predict(ridge.mod,s=50,type="coefficients")[1:20,] # gets coefficients

set.seed(1)
train=sample(1:nrow(x), nrow(x)/2) # random sample w/o replacement
test=(-train)
y.test=y[test]
# predicting MSE on a test set
# get coef w/ x, y, ridge regression, grid, and thresh.
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
mean((mean(y[train])-y.test)^2) # MSE if we predict mean y of training set
# We could get a similar MSE with a large lambda value (part of penalty term)
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,]) # predicts test set values
mean((ridge.pred-y.test)^2)
# Recall that least squares is simply ridge regression with lambda = 0
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T)
mean((ridge.pred-y.test)^2)
lm(y~x, subset=train)
predict(ridge.mod,s=0,exact=T,type="coefficients")[1:20,]

# It's better to use CV to choose lambda; cv.glmnet() is built in to do this.
# by default 10 folds are used
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)  ## Very good as quick Cross Validation!
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]

## The Lasso
# we use alpha=1 for lasso
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
# We now use CV
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
# 12 of the 19 coefficients are set to zero.
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef

## PCR
# uses pcr() function; pls library.  We use PCR to predict Salary
# be sure to remove the missing values
library(pls)
set.seed(2)
pcr.fit=pcr(Salary~., data=Hitters,scale=TRUE,validation="CV")
# pcr() function is similar to lm()
# scale=TRUE standardizes with xi <- xi / sqrt(distance from the mean)
# validation="CV" causes pcr() to compute the ten-fold CV error
summary(pcr.fit)
# we can visualize this with a plot
validationplot(pcr.fit,val.type="MSEP") # MSEP is CV MSE

set.seed(1)
pcr.fit=pcr(Salary~.,data=Hitters,subset=train,scale=TRUE,validation="CV")
validationplot(pcr.fit,val.type="MSEP")
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
# This is competitive with ridge regression and lasso
# It is less interpretable because we don't have coefficients or variable selection with PCR
pcr.fit=pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)

## Partial Least Squares (PLS) Regression
# plsr() of pls library
set.seed(1)
pls.fit=plsr(Salary~.,data=Hitters,subset=train,scale=TRUE,validation="CV")
summary(pls.fit)
validationplot(pls.fit,val.type="MSEP")
pls.pred=predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred-y.test)^2)

pls.fit=plsr(Salary~.,data=Hitters,scale=TRUE,ncomp=2)
summary(pls.fit)
# PLS is a supervised alternative to PCR